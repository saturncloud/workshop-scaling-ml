{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right\" src=\"img/saturn.png\" width=\"300\" />\n",
    "\n",
    "# Scaling Machine Learning in Python\n",
    "\n",
    "## Single-node workflow\n",
    "\n",
    "We'll first start off with a typical data preparation and machine learning workflow utilizing only the Jupyter Server.\n",
    "\n",
    "\n",
    "## Monitor resource utilization\n",
    "\n",
    "For this workshop it's important to monitor CPU and memory utilization when running various commands. It will help with understanding which operations are slow - and which ones run faster on a cluster!\n",
    "\n",
    "To monitor resource utilization of the Jupyter Server, open a new Terminal window inside Jupyter Lab and run `htop`. You can position the window to view the notebook and terminal on the same screen:\n",
    "\n",
    "![htop](img/htop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "This workshop will utilize data for yellow taxi rides from the 2019 calendar year. The machine learning exercises involve predicting the \"tip fraction\" of each ride - how much a rider will tip the driver as a fraction of the charged fare amount.\n",
    "\n",
    "Let's operate with one month for now to explore the data and build out the machine learning code.\n",
    "\n",
    "> Note: `s3.open()` is only required because we're using an anonymous S3 connection. If you have AWS credentials set up, you could pass in just the S3 URL string to `pd.read_csv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan2019_file = 's3://nyc-tlc/trip data/yellow_tripdata_2019-01.csv'\n",
    "jan2019_size = s3.du(jan2019_file)\n",
    "print(f\"Size: {round(jan2019_size / 1e6, 2)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "taxi = pd.read_csv(\n",
    "    s3.open(jan2019_file, mode='rb',),\n",
    "    parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "How many rows are in the `taxi` DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "len(taxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory usage is also an important consideration, as DataFrames often take more space in memory than on disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_bytes = taxi.memory_usage(deep=True).sum()\n",
    "print(f\"Size (MB): {taxi_bytes / 1e6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis\n",
    "\n",
    "For this workshop, we will just look at column statistics. There are many more explorary analyses that can performed with `pandas` and data visualization tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "np.round(taxi.describe().T, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "We are using stateless features, meaning the features values for a given observation don't depend on other observations. This is allows us to create features before performing any data splitting.\n",
    "\n",
    "Then, split data into train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify feature and label column names\n",
    "raw_features = [\n",
    "    'tpep_pickup_datetime', \n",
    "    'passenger_count', \n",
    "    'tip_amount', \n",
    "    'fare_amount',\n",
    "]\n",
    "features = [\n",
    "    'pickup_weekday', \n",
    "    'pickup_weekofyear', \n",
    "    'pickup_hour', \n",
    "    'pickup_week_hour', \n",
    "    'pickup_minute', \n",
    "    'passenger_count',\n",
    "]\n",
    "label = 'tip_fraction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_df(taxi_df):\n",
    "    '''\n",
    "    Generate features from a raw taxi dataframe.\n",
    "    '''\n",
    "    df = taxi_df[taxi_df.fare_amount > 0][raw_features].copy()  # avoid divide-by-zero\n",
    "    df[label] = df.tip_amount / df.fare_amount\n",
    "     \n",
    "    df['pickup_weekday'] = df.tpep_pickup_datetime.dt.weekday\n",
    "    df['pickup_weekofyear'] = df.tpep_pickup_datetime.dt.weekofyear\n",
    "    df['pickup_hour'] = df.tpep_pickup_datetime.dt.hour\n",
    "    df['pickup_week_hour'] = (df.pickup_weekday * 24) + df.pickup_hour\n",
    "    df['pickup_minute'] = df.tpep_pickup_datetime.dt.minute\n",
    "    df = df[features + [label]].astype(float).fillna(-1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_feat = prep_df(taxi)\n",
    "taxi_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    taxi_feat[features], \n",
    "    taxi_feat[label], \n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "\n",
    "We'll train a linear model to predict `tip_fraction`. We define a `Pipeline` to encompass both feature scaling and model training. This will be useful later when performing a grid search.\n",
    "\n",
    "Evaluate the model against the test set using RMSE. We'll also save out the model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('scale', StandardScaler()),\n",
    "    ('clf', ElasticNet(normalize=False, max_iter=100, l1_ratio=0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fitted = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preds = fitted.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, preds, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "\n",
    "with open('/tmp/model.pkl', 'wb') as f:\n",
    "    cloudpickle.dump(fitted, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooray!\n",
    "\n",
    "We trained a terrible model. But that's okay. The point of this workshop is to scale our work, not make the model perfect!\n",
    "\n",
    "# Let's step things up\n",
    "\n",
    "We were able to train a single, simple model on a sample of the taxi data (single month from 2019). In most model building settings more data and more compute would be required. We will explore the following scenarios where the single-node environment has challenges - and see how Dask solves these problems!\n",
    "\n",
    "- Tune hyperparameters (train numerous models)\n",
    "- Load and process large dataset (full year's data)\n",
    "- Predict over large dataset\n",
    "- Train model with large dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyperparameters\n",
    "\n",
    "One model can be trained comfortably and fairly quickly with the amount of memory and processing power the Jupyter Server has. A hyperparameter search involves training many models - which means it needs a lot of processing power to finish in a reasonable amount of time.\n",
    "\n",
    "Scikit-learn's `GridSearchCV` accepts an `n_jobs` argument, which will train `n` different models at the same time. Setting it to `-1` uses all the CPU cores in this Jupyter Server. Watch `htop` here - the cores will be saturated while memory will not be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'clf__l1_ratio': np.arange(0, 1.1, 0.1),\n",
    "    'clf__alpha': [0, 0.5, 1, 2],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, \n",
    "    params, \n",
    "    cv=3, \n",
    "    n_jobs=-1, \n",
    "    verbose=1, \n",
    "    scoring='neg_mean_squared_error',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a sample so this portion will execute faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_sample = taxi_feat.sample(frac=0.1, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = grid_search.fit(\n",
    "    taxi_sample[features],\n",
    "    taxi_sample[label],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This works, but...\n",
    "\n",
    "This is a pretty small grid search. With a larger grid, it would take quite a while.\n",
    "\n",
    "![waiting](https://media3.giphy.com/media/QBd2kLB5qDmysEXre9/giphy.gif?cid=ecf05e47t2wdryi59apjgwa3okzhxpctvxjsqaxautawn96i&rid=giphy.gif)\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process large dataset\n",
    "\n",
    "Let's look at the size of the files on disk using `s3fs`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = s3.glob('s3://nyc-tlc/trip data/yellow_tripdata_2019-*.csv')\n",
    "total_size = 0\n",
    "for f in files:\n",
    "    size = s3.du(f)\n",
    "    total_size += size\n",
    "    \n",
    "    print(f\"{f}, Size: {round(size / 1e6, 2)} MB\")\n",
    "print()\n",
    "print(f\"Total size: {round(total_size / 1e9, 2)} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Files end up taking more space when loaded into memory, but let's see if this will fit on our Jupyter Server. We can loop through the files and concatenate them into one DataFrame. Watch memory utilization as the loop runs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(file):\n",
    "    df = pd.read_csv(\n",
    "        s3.open(file, mode='rb'),\n",
    "        parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime']\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dfs = []\n",
    "for f in files:\n",
    "    print(f)\n",
    "    df = load_csv(f)\n",
    "    print(f'{len(df)} rows, {df.memory_usage(deep=True).sum() / 1e6} MB')\n",
    "    dfs.append(df)\n",
    "taxi_big = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![explosion](https://media4.giphy.com/media/13d2jHlSlxklVe/giphy.gif)\n",
    "\n",
    "Oh no! Looks like we have enough memory to load the CSV files individually, but not to concatenate them together into one DataFrame.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict over large dataset\n",
    "\n",
    "We can't load a large dataset into one DataFrame, but if we needed to predict over a large dataset we could batch it and collect the results.\n",
    "\n",
    "> Note: If your kernel died when trying to concatenate the big DataFrame, you will need to re-run all the preceding cells in this notebook - just not the one that killed the kernel!\n",
    "\n",
    "This will work, but watch `htop` - only one core will be used at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predicted_dfs = []\n",
    "for f in files:\n",
    "    print(f)\n",
    "    df = load_csv(f)\n",
    "    feat_df = prep_df(df)\n",
    "    predicted = fitted.predict(feat_df[features])\n",
    "    predicted_df = pd.DataFrame(predicted, columns=['prediction'])\n",
    "    predicted_dfs.append(predicted_df)\n",
    "    \n",
    "predicted_full = pd.concat(predicted_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_full.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Train model with large dataset\n",
    "\n",
    "We are stuck here, because we need to be able to load the full dataset into memory to train with it.\\*\n",
    "\n",
    "Think about all the data we're missing out on. All those models, all those hyperparameters that will never get a chance!\n",
    "\n",
    "<img src=\"https://media0.giphy.com/media/k61nOBRRBMxva/giphy.gif\" width=\"400\" alt=\"crying\" />\n",
    "\n",
    "_\\* Scikit-learn supports [incremental learning](https://scikit-learn.org/stable/modules/computing.html#incremental-learning) for models that have a `partial_fit` method. `ElasticNet` does not have a `partial_fit` method._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There must be a better way!\n",
    "\n",
    "<img src=\"https://docs.dask.org/en/latest/_images/dask_horizontal_no_pad.svg\" width=\"300\" alt=\"dask\" />\n",
    "\n",
    "With Dask, we can scale out to a cluster to address these four failure scenarios. \n",
    "\n",
    "Move on to [03-hyperparameter.ipynb](03-hyperparameter.ipynb) to get started!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right\" src=\"img/saturn.png\" width=\"300\" />\n",
    "\n",
    "# Scaling Machine Learning in Python\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "The hyperparameter search from [02-single-node.ipynb](02-single-node.ipynb) is an example of a compute-bound workload. The data fits comfortably into memory of the Jupyter Server, but the grid search still takes some time to execute. Let's take this workflow and parallelize it with Dask!\n",
    "\n",
    "## Initialize Dask cluster\n",
    "\n",
    "The `dask_saturn` package makes the Dask Cluster that we created from Saturn Cloud accessible in our notebook. If the cluster was already created, we would not need to specify any arguments when initializing `SaturnCluster`, but it is a good idea to do so for reproducibility purposes. The arguments to `SaturnCluster` match the fields presented when editing a Dask Cluster from the Saturn Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_saturn import SaturnCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = SaturnCluster(\n",
    "    scheduler_size='medium',\n",
    "    worker_size='xlarge',\n",
    "    n_workers=3,\n",
    "    nthreads=4,\n",
    ")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "To see the options for scheduler and worker sizes, and how they match up to the options presented in Saturn Cloud, run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_saturn.core import describe_sizes\n",
    "describe_sizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The `Client` object is our \"entry point\" to Dask. Most Dask operations will automatically detect the client and run operations across the cluster, but sometimes its necessary to pass a `client` object when performing more advanced operations. Previewing the `client` object tells us details about the cluster and a link to the Dashboard. Open up the Dashboard now and keep it  visible in a separate window - you'll see it light up when we run Dask operations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will block until all workers are available. You can also view cluster status and access the Dashboard link from the Project page in Saturn Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.wait_for_workers(3)\n",
    "print('Ready to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the number of workers on the running cluster using the `cluster.scale()` method. Note that it will take a few minutes to spin up new workers, but you can use the above `wait_for_workers()` function to block until they're ready:\n",
    "\n",
    "```python\n",
    "cluster.scale(10) # more workers!\n",
    "client.wait_for_workers(10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Scikit-learn + Joblib\n",
    "\n",
    "Scikit-learn has some algorithms that support parallel execution via the `n_jobs` parameter. By default, this parallelizes across all cores on a single machine (in this case, our Jupyter Server). Dask provides a [Joblib backend](https://ml.dask.org/joblib.html) that hooks into scikit-learn algorithms to parallelize work across a Dask cluster. This enables us to pull in Dask just for the grid search.\n",
    "\n",
    "In this case, all data loading and processing code is exactly the same as [02-single-node.ipynb](02-single-node.ipynb) and executes on the Jupyter Server. The only part that executes in a Dask cluster is the grid search execution.\n",
    "\n",
    "The following cell will take a few seconds to execute, but everything in it is copied from [02-single-node.ipynb](02-single-node.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "taxi = pd.read_csv(\n",
    "    s3.open(\n",
    "        's3://nyc-tlc/trip data/yellow_tripdata_2019-01.csv',\n",
    "        mode='rb',\n",
    "    ),\n",
    "    parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime']\n",
    ")\n",
    "\n",
    "raw_features = [\n",
    "    'tpep_pickup_datetime', \n",
    "    'passenger_count', \n",
    "    'tip_amount', \n",
    "    'fare_amount',\n",
    "]\n",
    "features = [\n",
    "    'pickup_weekday', \n",
    "    'pickup_weekofyear', \n",
    "    'pickup_hour', \n",
    "    'pickup_week_hour', \n",
    "    'pickup_minute', \n",
    "    'passenger_count',\n",
    "]\n",
    "label = 'tip_fraction'\n",
    "\n",
    "def prep_df(taxi_df):\n",
    "    '''\n",
    "    Generate features from a raw taxi dataframe.\n",
    "    '''\n",
    "    df = taxi_df[taxi_df.fare_amount > 0][raw_features].copy()  # avoid divide-by-zero\n",
    "    df[label] = df.tip_amount / df.fare_amount\n",
    "     \n",
    "    df['pickup_weekday'] = df.tpep_pickup_datetime.dt.weekday\n",
    "    df['pickup_weekofyear'] = df.tpep_pickup_datetime.dt.weekofyear\n",
    "    df['pickup_hour'] = df.tpep_pickup_datetime.dt.hour\n",
    "    df['pickup_week_hour'] = (df.pickup_weekday * 24) + df.pickup_hour\n",
    "    df['pickup_minute'] = df.tpep_pickup_datetime.dt.minute\n",
    "    df = df[features + [label]].astype(float).fillna(-1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "taxi_feat = prep_df(taxi)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('scale', StandardScaler()),\n",
    "    ('clf', ElasticNet(normalize=False, max_iter=100, l1_ratio=0)),\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'clf__l1_ratio': np.arange(0, 1.1, 0.1),\n",
    "    'clf__alpha': [0, 0.5, 1, 2],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, \n",
    "    params, \n",
    "    cv=3, \n",
    "    n_jobs=-1,\n",
    "    verbose=1, \n",
    "    scoring='neg_mean_squared_error',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a sample to match what we did in [02-single-node.ipynb](02-single-node.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_sample = taxi_feat.sample(frac=0.1, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute the grid search in Dask we need to run inside a context manager for a Joblib backend. Besides that, we call the `grid_search.fit()` method the same way as before. When you run this cell, watch the Dask Dashboard to see the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import joblib\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    _ = grid_search.fit(\n",
    "        taxi_sample[features],\n",
    "        taxi_sample[label],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success! \n",
    "\n",
    "That executed in half the time as [02-single-node.ipynb](02-single-node.ipynb), just by initializing a Dask Cluster and adding one line of code. If we had more nodes in the cluster it would execute even faster.\n",
    "\n",
    "> **Note**: Using the Dask Joblib backend requires sending the DataFrame through the scheduler to all the workers. This causes problems with DataFrames larger than what was used in this example.\n",
    "\n",
    "Dask + Joblib is useful for small data scenarios. Our next example will work with DataFrames of any size!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask ML\n",
    "\n",
    "Dask ML has its own parallel implementations of some scikit-learn algorithms, including `GridSearchCV` and [other hyperparameter search options](https://ml.dask.org/hyper-parameter-search.html). To use it, we convert our pandas DataFrame to a Dask DataFrame and use Dask ML's preprocessing and model selection classes. Don't worry if you don't know all the details of a Dask DataFrame -  we'll cover that in the next notebook. For now, enjoy the speedups with Dask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "\n",
    "taxi_sample_dd = (\n",
    "    dd.from_pandas(taxi_sample, npartitions=10)\n",
    "    .persist()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the following code looks almost identical to the scikit-learn version, and even still uses scikit-learn's `Pipeline` and `ElasticNet` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from dask_ml.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('scale', StandardScaler()),\n",
    "    ('clf', ElasticNet(normalize=False, max_iter=100, l1_ratio=0)),\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'clf__l1_ratio': np.arange(0, 1.1, 0.1),\n",
    "    'clf__alpha': [0, 0.5, 1, 2],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, \n",
    "    params, \n",
    "    cv=3, \n",
    "    scoring='neg_mean_squared_error',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Run the grid search using the `grid_search` object defined above. (Hint: it works the same way as scikit-learn's `GridSearchCV` class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = grid_search.fit(\n",
    "    taxi_sample_dd[features], \n",
    "    taxi_sample_dd[label],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### Super fast!\n",
    "\n",
    "This ran even faster than the Joblib example, because Dask was able to parallelize all steps of the pipeline. Dask ML's `GridSearchCV` class also [avoids repeated work](https://ml.dask.org/hyper-parameter-search.html#avoid-repeated-work) to make the grid search faster. \n",
    "\n",
    "Jump over to [04-large-dataset.ipynb](04-large-dataset.ipynb) for a deeper dive into Dask and see how we can do batch inference and model training with large amounts of data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

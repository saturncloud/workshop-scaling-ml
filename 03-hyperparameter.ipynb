{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right\" src=\"img/saturn.png\" width=\"300\" />\n",
    "\n",
    "# Scaling Machine Learning in Python\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "The hyperparameter search from [02-single-node.ipynb](02-single-node.ipynb) is an example of a compute-bound workload. The data fits comfortably into memory of the Jupyter Server, but the grid search still takes some time to execute. Let's take this workflow and parallelize it with Dask!\n",
    "\n",
    "## Initialize Dask cluster\n",
    "\n",
    "The `dask_saturn` package makes the Dask Cluster that we created from Saturn Cloud accessible in our notebook. If the cluster was already created, we would not need to specify any arguments when initializing `SaturnCluster`, but it is a good idea to do so for reproducibility purposes. The arguments to `SaturnCluster` match the fields presented when editing a Dask Cluster from the Saturn Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-11-09 21:54:14] WARNING - dask-saturn | Unable to update the specification of a running dask cluster. No changes applied. To update, first stop the cluster with `cluster.close()` and reinitialize, or run 'cluster = SaturnCluster.reset(**kwargs)' to restart automatically.\n",
      "[2020-11-09 21:54:14] INFO - dask-saturn | Cluster is ready\n"
     ]
    }
   ],
   "source": [
    "from dask_saturn import SaturnCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = SaturnCluster(\n",
    "    scheduler_size='medium',\n",
    "    worker_size='xlarge',\n",
    "    n_workers=3,\n",
    "    nthreads=4,\n",
    ")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The `Client` object is our \"entry point\" to Dask. Most Dask operations will automatically detect the client and run operations across the cluster, but sometimes its necessary to pass a `client` object when performing more advanced operations. Previewing the `client` object tells us details about the cluster and a link to the Dashboard. Open up the Dashboard now and watch it in a separate window - you'll see it light up when we run Dask operations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://d-aaron-dask-workshop.main-namespace:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='https://d-aaron-dask-workshop.community.saturnenterprise.io' target='_blank'>https://d-aaron-dask-workshop.community.saturnenterprise.io</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>5</li>\n",
       "  <li><b>Cores: </b>20</li>\n",
       "  <li><b>Memory: </b>157.50 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.0.22.233:8786' processes=5 threads=20, memory=157.50 GB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will block until all workers are available. You can also view cluster status and access the Dashbaord link from the Project page in Saturn Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to go!\n"
     ]
    }
   ],
   "source": [
    "client.wait_for_workers(3)\n",
    "print('Ready to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Scikit-learn + Joblib\n",
    "\n",
    "Scikit-learn has some algorithms that support parallel execution via the `n_jobs` parameter. By default, this parallelizes across all cores on a single machine (in this case, our Jupyter Server). Dask provides a [Joblib backend](https://ml.dask.org/joblib.html) that hooks into scikit-learn algorithms to parallelize work across a Dask cluster. This enables us to pull in Dask just for the grid search.\n",
    "\n",
    "In this case, all data loading and processing code is exactly the same as [02-single-node.ipynb](02-single-node.ipynb) and executes on the Jupyter Server. The only part that executes in a Dask cluster is the grid search execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = pd.read_csv(\n",
    "    s3.open(\n",
    "        's3://nyc-tlc/trip data/yellow_tripdata_2019-01.csv',\n",
    "        mode='rb',\n",
    "    ),\n",
    "    parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_features = [\n",
    "    'tpep_pickup_datetime', \n",
    "    'passenger_count', \n",
    "    'tip_amount', \n",
    "    'fare_amount',\n",
    "]\n",
    "features = [\n",
    "    'pickup_weekday', \n",
    "    'pickup_weekofyear', \n",
    "    'pickup_hour', \n",
    "    'pickup_week_hour', \n",
    "    'pickup_minute', \n",
    "    'passenger_count',\n",
    "]\n",
    "label = 'tip_fraction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_df(taxi_df):\n",
    "    '''\n",
    "    Generate features from a raw taxi dataframe.\n",
    "    '''\n",
    "    df = taxi_df[taxi_df.fare_amount > 0][raw_features].copy()  # avoid divide-by-zero\n",
    "    df[label] = df.tip_amount / df.fare_amount\n",
    "     \n",
    "    df['pickup_weekday'] = df.tpep_pickup_datetime.dt.weekday\n",
    "    df['pickup_weekofyear'] = df.tpep_pickup_datetime.dt.weekofyear\n",
    "    df['pickup_hour'] = df.tpep_pickup_datetime.dt.hour\n",
    "    df['pickup_week_hour'] = (df.pickup_weekday * 24) + df.pickup_hour\n",
    "    df['pickup_minute'] = df.tpep_pickup_datetime.dt.minute\n",
    "    df = df[features + [label]].astype(float).fillna(-1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_feat = prep_df(taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('scale', StandardScaler()),\n",
    "    ('clf', ElasticNet(normalize=False, max_iter=100, l1_ratio=0)),\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'clf__l1_ratio': np.arange(0, 1.1, 0.1),\n",
    "    'clf__alpha': [0, 0.5, 1, 2],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, \n",
    "    params, \n",
    "    cv=3, \n",
    "    n_jobs=-1,\n",
    "    verbose=1, \n",
    "    scoring='neg_mean_squared_error',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a sample to match what we did in [02-single-node.ipynb](02-single-node.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_sample = taxi_feat.sample(frac=0.1, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute it in Dask we need to run inside a context manager for a Joblib backend. Besides that, we call the `grid_search.fit()` method the same way as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 44 candidates, totalling 132 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend DaskDistributedBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    8.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.87 s, sys: 2.05 s, total: 3.92 s\n",
      "Wall time: 14.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 132 out of 132 | elapsed:   14.3s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import joblib\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    _ = grid_search.fit(\n",
    "        taxi_sample[features],\n",
    "        taxi_sample[label],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success! \n",
    "\n",
    "That executed in half the time as [02-single-node.ipynb](02-single-node.ipynb), just by initializing a Dask Cluster and adding one line of code. If we had more nodes in the cluster it would execute even faster.\n",
    "\n",
    "> **Note**: Using the Dask Joblib backend requires sending the DataFrame through the scheduler to all the workers. This causes problems with DataFrames larger than what was used in this example.\n",
    "\n",
    "Dask + Joblib is useful for small data scenarios. Our next example will work with DataFrames of any size!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask ML\n",
    "\n",
    "Dask ML has its own parallel implementations of some scikit-learn algorithms, including `GridSearchCV` and [other hyperparameter search options](https://ml.dask.org/hyper-parameter-search.html). To use it, we convert our DataFrame to a Dask DataFrame and use Dask ML's preprocessing classes. Don't worry if you don't know all the details of a Dask DataFrame -  we'll cover that in the next notebook. For now, enjoy the speedups with Dask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "\n",
    "taxi_sample_dd = (\n",
    "    dd.from_pandas(taxi_sample, npartitions=10)\n",
    "    .persist()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how this code looks almost identical to the scikit-learn version, and even still uses scikit-learn's `Pipeline` and `ElasticNet` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from dask_ml.metrics import mean_squared_error\n",
    "from dask_ml.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('scale', StandardScaler()),\n",
    "    ('clf', ElasticNet(normalize=False, max_iter=100, l1_ratio=0)),\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'clf__l1_ratio': np.arange(0, 1.1, 0.1),\n",
    "    'clf__alpha': [0, 0.5, 1, 2],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, \n",
    "    params, \n",
    "    cv=3, \n",
    "    scoring='neg_mean_squared_error',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 160 ms, sys: 27.4 ms, total: 187 ms\n",
      "Wall time: 8.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = grid_search.fit(\n",
    "    taxi_sample_dd[features], \n",
    "    taxi_sample_dd[label],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Super fast!\n",
    "\n",
    "This ran even faster than the Dask + Joblib example, because Dask was able to parallelize all steps of the pipeline. Dask ML's `GridSearchCV` class also [avoids repeated work](https://ml.dask.org/hyper-parameter-search.html#avoid-repeated-work) to make the grid search faster. \n",
    "\n",
    "Jump over to [04-large-dataset.ipynb](04-large-dataset.ipynb) for a deeper dive into Dask and see how we can do batch inference and model training with large amounts of data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
